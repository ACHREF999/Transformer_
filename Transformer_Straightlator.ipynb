{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwTnLS0YCrjM",
        "outputId": "5e6ebb68-876f-43fd-f03c-e6b9fe644c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "runs  sample_data  tokenizer_en.json  tokenizer_fr.json  weights\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        \"\"\"\n",
        "        d_model is the dimension of the vector,\n",
        "\n",
        "        vocab_size is the number of words in the vocbulary\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        \"given a number always given a number it proiveds the  same number every time , \"\n",
        "        \"embedding is mapping between number and a vector of size d_model\"\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        \"this is just a dictionnary\"\n",
        "    def forward(self, x):\n",
        "        \"the vector the number being mapped to is learned by the model \"\n",
        "        \"in the paper they mult by sqrt(d_model)\"\n",
        "        return self.embedding(x)*math.sqrt(self.d_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model,seq_len,dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # we will be creating pos enco for seq_len each with 512 size vec\n",
        "        \"so we just return a matrix\"\n",
        "        pos_enc = torch.zeros(seq_len,d_model)\n",
        "        \"positions vector (seq_len,1)\"\n",
        "        position = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n",
        "        determinator = torch.exp(torch.arange(0,d_model,2).float()*(-math.log(1000)/d_model))\n",
        "        # all tokens , all rows from 0 to the end jumping by two\n",
        "        # the even pos use the sine function\n",
        "        pos_enc[:,0::2]= torch.sin(position*determinator)\n",
        "\n",
        "        # all tokens , all rows from 1 to the end jumping by two\n",
        "        # the odd pos use the cosine function\n",
        "        pos_enc[:,1::2]= torch.cos(position*determinator)\n",
        "\n",
        "        # for the model we deal with batches so we need the batch dimension\n",
        "        pos_enc = pos_enc.unsqueeze(0)\n",
        "\n",
        "\n",
        "        # when saving the model this buffer will be saved along with the state of the model\n",
        "        # why? to store fixed non trainable data\n",
        "        self.register_buffer('pos_enc',pos_enc)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # they are fixed not learned\n",
        "        # i think when backproping we will only go through the x\n",
        "        x = x + self.pos_enc[:,:x.shape[1],:].requires_grad_(False)\n",
        "\n",
        "        # dropout reduce the reliance of the model on the position\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "# layer normalization :\n",
        "# when we have three items , each item has  mean and std\n",
        "# we normalize each item with its mean and std\n",
        "# we introduce in the normalization two params additive and multiplicative\n",
        "# normalization can hurt us when the fluctuation is actually necessary\n",
        "# where it happens  :\n",
        "# Not across batch: Unlike BatchNorm, LayerNorm ignores other samples.\n",
        "# Not across tokens: It does not normalize across the sequence (tokens).\n",
        "# Only across embedding features (d_model) for each token.\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self,eps = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # cpus or gpus can only represent numbers within a scale ,\n",
        "        # by introducing the eps we are making sure we are setting\n",
        "        # a ceiling on the normalized embedding item\n",
        "        self.alpha = nn.Parameter(torch.ones(1)) # multiplicative param\n",
        "        self.bias = nn.Parameter(torch.ones(1)) # additive param\n",
        "        # maybe instead of 1 it should be d_model\n",
        "        # so that each embedding of the token moves on its own\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # Output shape: (batch_size, seq_len, 1)\n",
        "\n",
        "        # Each [b, s, :] vector (one token) is reduced to a single mean value.\n",
        "\n",
        "        # keepdim=True ensures the output can still broadcast with x during subtraction.\n",
        "        # if we dont use keepdim=True then\n",
        "        # This won’t work because you're trying to subtract a\n",
        "        # (batch_size, seq_len) tensor from\n",
        "        # a (batch_size, seq_len, d_model) tensor. The dimensions don’t line up for broadcasting.\n",
        "        # Because now you're subtracting a (batch_size, seq_len, 1) tensor from\n",
        "        # a (batch_size, seq_len, d_model) tensor —\n",
        "        # PyTorch automatically stretches the 1 to match d_model.\n",
        "        # It computes the mean across the last dimension —\n",
        "        # that is, across the embedding dimension (d_model) — for each token.\n",
        "        mean = x.mean(dim=-1,keepdim=True)\n",
        "        std = x.std(dim=-1,keepdim=True)\n",
        "        normalized = (x-mean) / std+self.eps\n",
        "        return self.alpha*normalized + self.bias\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,d_model,dff,dropout):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model,dff) # W1 B1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dff,d_model) # W2 B2\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # (Batch,seq_len,d_model) --> (Batch,seq_len,dff) --> (Batch,seq_len,d_model)\n",
        "        x = self.linear1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self,d_model , h , dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        assert d_model%h == 0 ,\"d_model is not div by h\"\n",
        "\n",
        "        self.d_k = d_model//h\n",
        "\n",
        "        self.w_q = nn.Linear(d_model,d_model,bias=False)\n",
        "        self.w_k = nn.Linear(d_model,d_model,bias=False)\n",
        "        self.w_v = nn.Linear(d_model,d_model , bias= False)\n",
        "\n",
        "        self.w_o = nn.Linear(d_model,d_model,bias=False)\n",
        "\n",
        "        # Default\n",
        "\n",
        "        # self.w_q = nn.Linear(d_model,d_model)\n",
        "        # self.w_k = nn.Linear(d_model,d_model)\n",
        "        # self.w_v = nn.Linear(d_model,d_model)\n",
        "\n",
        "        # self.w_o = nn.Linear(d_model,d_model)\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(q,k,v,mask,dropout):\n",
        "\n",
        "        d_k = q.shape[-1]\n",
        "        # @ is for mat mul\n",
        "        # remember that this mul is :\n",
        "        # doing matrix multiplication per head, per batch, between:\n",
        "        # query vectors and\n",
        "        # key vectors (transposed)\n",
        "        # this arrangement/order is so that we have a meaningfull arch\n",
        "        # for each query get the best fitting key and propagate its value\n",
        "\n",
        "        # (batch , h , seq_len,d_k) --> (batch, h, seq_len,seq_len)\n",
        "        attention_scores = (q @ k.transpose(-2,-1))/math.sqrt(d_k)\n",
        "\n",
        "        #  this is where we utilize the mask\n",
        "        if mask is not None :\n",
        "            attention_scores.masked_fill_(mask==0,-1e9)\n",
        "\n",
        "        # (batch,h,seq_len,seq_len)\n",
        "        #  remember that softmax is not per scalar but per vector\n",
        "        #  we use softmax over all keys per query ,\n",
        "        attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "        if dropout is not None :\n",
        "            attention_scores = dropout(attention_scores)\n",
        "\n",
        "\n",
        "        #  the actual output is x\n",
        "        x = attention_scores @ v\n",
        "        return x ,attention_scores\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,q,k,v,mask):\n",
        "        # so when we dont want some tokens to interract with each other\n",
        "        # we can implement that by using a mask\n",
        "        # we leverage the fact that we are using a softmax to `pick up`\n",
        "        # the values so we intercept the calculation of the attention\n",
        "        # to remove the attention of certain tokens\n",
        "        # the buffer we can intercept is the Q*Kt matrix\n",
        "        # which is (d_model,d_model)\n",
        "        query = self.w_q(q) # (batch,seq_len,d_model) --> (batch,seq_len,d_model)\n",
        "        key = self.w_k(k) # (batch,seq_len,d_model) --> (batch,seq_len,d_model)\n",
        "        value = self.w_v(v) # (batch,seq_len,d_model) --> (batch,seq_len,d_model)\n",
        "\n",
        "        #  now we split into head chunks\n",
        "        # (batch,seq_len,d_model) --> (batch,seq_len,h,d_k) ->(batch,h,seq_len,d_k)\n",
        "        query = query.view(query.shape[0],query.shape[1],self.h,self.d_k).transpose(1,2)\n",
        "        #  the transpose is so we could see the whole sentence head\n",
        "        #  the transpose(1,2) swaps dims 1 and 2\n",
        "        #  this will make the calculation much easier\n",
        "        #  also each head will see (seq_len , d_k)\n",
        "        #  all of the sentence and a subset of the embeddings\n",
        "        #  (batch,seq_len,d_model)\n",
        "        key = key.view(key.shape[0],key.shape[1],self.h,self.d_k).transpose(1,2)\n",
        "        value = value.view(value.shape[0],value.shape[1],self.h,self.d_k).transpose(1,2)\n",
        "\n",
        "        x,self.attention_scores = MultiHeadAttentionBlock.attention(query,key,value,mask,self.dropout)\n",
        "        # reverse the transpose\n",
        "        # (batch,h,seq_len,d_k) --> (batch,seq_len,h,d_k) --> (batch,seq_len,d_model)\n",
        "        x = x.transpose(1,2).contiguous().view(x.shape[0],-1,self.h*self.d_k)\n",
        "\n",
        "        # This is required because .transpose() returns a non-contiguous tensor\n",
        "        # — basically, it messes with memory layout.\n",
        "        # You can’t use .view() unless the tensor is contiguous in memory.\n",
        "\n",
        "        # batch_size = x.shape[0]\n",
        "\n",
        "        # seq_len = inferred from shape (because of -1)\n",
        "\n",
        "        # self.h * self.d_k = d_model\n",
        "\n",
        "\n",
        "        # (batch , seq_len,d_model) --> (batch,seq_len,d_model)\n",
        "        x = self.w_o(x)\n",
        "\n",
        "        return x\n",
        "        # con.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# This is the equivalent of the add&norm\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self,dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNorm()\n",
        "\n",
        "\n",
        "    def forward(self,x,sublayer):\n",
        "        # in the paper they apply norm before the sublayer\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    # self attention because the input is applied in 3 different roles\n",
        "    # the input is all the query , the key and the value\n",
        "    def __init__(self,self_attention_block,feed_forward_block,dropout):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.dropout = dropout\n",
        "        self.residual_connections = nn.ModuleList(\n",
        "            [\n",
        "                ResidualConnection(self.dropout),\n",
        "                ResidualConnection(self.dropout),\n",
        "            ])\n",
        "\n",
        "\n",
        "    def forward(self , x,src_mask) :\n",
        "        # this mask is to hide the interraction between padding words\n",
        "        # and actual words\n",
        "        # self attention : input is all query key value\n",
        "        # cros attention : querys from dec are watching k , v from enc\n",
        "        attention_sublayer = lambda x : self.self_attention_block(x,x,x,src_mask)\n",
        "\n",
        "        x = self.residual_connections[0](x,attention_sublayer)\n",
        "\n",
        "        feed_forward_sublayer  = lambda x : self.feed_forward_block(x)\n",
        "\n",
        "        x = self.residual_connections[1](x,feed_forward_sublayer)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,layers:nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNorm()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x,mask):\n",
        "\n",
        "        for layer in self.layers :\n",
        "            x = layer(x,mask)\n",
        "\n",
        "\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self,self_attention_block , cross_attention_block,feed_forward_block ,dropout):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.dropout= dropout\n",
        "        self.residual_connections = nn.ModuleList([\n",
        "            ResidualConnection(dropout),\n",
        "            ResidualConnection(dropout),\n",
        "            ResidualConnection(dropout),\n",
        "\n",
        "        ])\n",
        "\n",
        "\n",
        "    def forward(self,x , encoder_output,src_mask,tgt_mask):\n",
        "        #\n",
        "        # the src_mask is used by the decoder when looking into the encoder\n",
        "        # so it ignores the padding tokens,\n",
        "        # the tgt_mask is used by the decoder when looking into the\n",
        "        # decoder input so it doesnt cheat , and can learn\n",
        "        # --------------------------------------------------------------\n",
        "        # so in general any decoder using self-attention needs tgt_mask\n",
        "        # to mask future and force the autoregressive prediction\n",
        "        #\n",
        "        # any encoder_decoder attention needs the src_mask so it\n",
        "        # knows what to not look at from the source (encoder)\n",
        "        # basically ignore the padded tokens in the source\n",
        "\n",
        "        # now we are only masking the future of the output embeddings\n",
        "        self_attention_sublayer = lambda x: self.self_attention_block(x,x,x\n",
        "                                                                      ,tgt_mask)\n",
        "        x = self.residual_connections[0](x,self_attention_sublayer)\n",
        "\n",
        "        # now we need to mask the padding from the encoder\n",
        "        # the reason we don't need tgt in here is because\n",
        "        # the query is already built upon masking\n",
        "        # which means now we only need the\n",
        "        # src_msking\n",
        "        cross_attention_sublayer = lambda x : \\\n",
        "                self.cross_attention_block(\n",
        "                                x,\n",
        "                                encoder_output,\n",
        "                                encoder_output,\n",
        "                                src_mask)\n",
        "\n",
        "        x = self.residual_connections[1](x,cross_attention_sublayer)\n",
        "\n",
        "        feed_forward_sublayer = lambda x : self.feed_forward_block(x)\n",
        "        x = self.residual_connections[2](x,feed_forward_sublayer)\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        so we agree that there is some waste of memory throught all the encoder's attention blocks :\n",
        "        (batch , seq_len_usefull +seq_len_useless, d_model ) .\n",
        "        now if the decoder also propagates useless information through masking shouldnt we keep on masking that vector across the decoder attention layers ?\n",
        "        then at the cross attention we will need both src_mask and tgt_mask ?\n",
        "\n",
        "        but why in every attention block in the decoder we keep using the tgt_mask?\n",
        "        wouldnt we stop information from propagating ?\n",
        "\n",
        "        so does that mean in generation ,\n",
        "        We are using only the attention of existing tokens to generate the new token ?\n",
        "        meaning we are trying extract next word soely from existing words ?\n",
        "        and that is why we train the model this way ? so it learns this behaviour ?\n",
        "\n",
        "        so in the decoder the future tokens play the role as padding\n",
        "        since they take space and we dont use their info in any way\n",
        "\n",
        "\n",
        "        since we are training in batches , we keep that wasted **per step** future\n",
        "        tokens which keep decresing as we consume the batch\n",
        "\n",
        "        in the decoder we have a wavy space loss\n",
        "        in encoder we have a constant space loss\n",
        "        but this loss is  tradeoff for training speed\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,layers):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNorm()\n",
        "\n",
        "\n",
        "    def forward(self,x,encoder_output,src_mask,tgt_mask):\n",
        "        for layer in self.layers :\n",
        "            x = layer(x,encoder_output,src_mask,tgt_mask)\n",
        "\n",
        "\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now at the end we get (batch , seq_len,d_model)\n",
        "# but we are not really intrested in the embeddings\n",
        "# so we want to somehow project the embeddings bck to words\n",
        "\n",
        "\n",
        "# so basically the ids of the vocabulary are the same for the input and output\n",
        "# remember that in input embeddings we go : eg.'cat'->voacb_indices(eg. 25)->embedding\n",
        "# in the projection layer (transformer output) we learn\n",
        "# mapping from output_embeddings->vocab_indices(eg. 29)-> eg. 'eats'\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self,d_model,vocab_size):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model,vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # this is to use log_probs instead of probs\n",
        "        # to escpe numericl underflow\n",
        "        # because proba*proba*proba ...vocab_size times is tooo close to 0\n",
        "        # we use log on the proba\n",
        "        # and we get a sum of some neg numbers which is managable\n",
        "        # remember that log(0.0001) ~ - 11\n",
        "        # so we escape underflow and overflow\n",
        "        # and we can still apply  the same comparaison\n",
        "        # take the biggest probability is the same as take biggest log_probability\n",
        "        # because log_probability is monotonic\n",
        "\n",
        "        # (batch,seq_len , d_model) --> (batch, seq_len , vocab_size)\n",
        "\n",
        "        # -The seq_len dimension persists because each token in the sequence gets\n",
        "        # its own individual log-probability distribution over the entire vocabulary.\n",
        "        # -The operations (linear projection and softmax) are applied independently to\n",
        "        # each token, and they don’t alter the number of tokens in the sequence.\n",
        "        # -Each token is processed separately, and for each token, you are generating\n",
        "        # a vector of size vocab_size (the log-probabilities for all words in the\n",
        "        # vocabulary). Thus, the sequence length (seq_len) remains intact.\n",
        "\n",
        "\n",
        "        # right before we go to project to vocab size , that embedding or logit or\n",
        "        # whatever u call it is the embedding of the generated word ?\n",
        "        # meaning we are morphing an embedding through many layers using context\n",
        "        # into an output embedding which will be mapped into a word ?\n",
        "        return torch.log_softmax(self.proj(x),dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "# CHECKPOINT\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,encoder , decoder,src_embedding,tgt_embedding,src_pos,tgt_pos,projection_layer):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embedding = src_embedding\n",
        "        self.tgt_embedding = tgt_embedding\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        # maybe each language has its own posit encod\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "\n",
        "    def encode(self,src,src_mask):\n",
        "        src = self.src_embedding(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src,src_mask)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def decode(self,encoder_output,src_mask,tgt,tgt_mask):\n",
        "        tgt = self.tgt_embedding(tgt)\n",
        "        tgt= self.tgt_pos(tgt)\n",
        "        #  remember this is not just one attention block its the whole DECODER!\n",
        "        # thats why we pass both the src_msk nd tgt_mask\n",
        "        tgt = self.decoder(tgt,encoder_output,src_mask,tgt_mask)\n",
        "        return tgt\n",
        "\n",
        "\n",
        "    def project(self,x):\n",
        "        return self.projection_layer(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_transformer(src_vocab_size,tgt_vocab_size,src_seq_len,tgt_seq_len,d_model=512,N=6,h=8,dropout=0.1,d_ff=2048):\n",
        "    src_embedding = InputEmbeddings(d_model,src_vocab_size)\n",
        "    tgt_embedding = InputEmbeddings(d_model,tgt_vocab_size)\n",
        "\n",
        "    src_pos = PositionalEncoding(src_seq_len,d_model,dropout)\n",
        "    tgt_pos = PositionalEncoding(tgt_seq_len,d_model,dropout)\n",
        "\n",
        "\n",
        "    # N encoder blocks\n",
        "    encoder_blocks = []\n",
        "\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention = MultiHeadAttentionBlock(d_model,h,dropout)\n",
        "        # remember that dff is the a ff upwards projection size\n",
        "        feed_forward = FeedForward(d_model,d_ff,dropout)\n",
        "        encoder_block = EncoderBlock(encoder_self_attention,feed_forward,dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # N decoder blocks\n",
        "    decoder_blocks = []\n",
        "\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention = MultiHeadAttentionBlock(d_model,h,dropout)\n",
        "        decoder_cross_attention = MultiHeadAttentionBlock(d_model,h,dropout)\n",
        "        feed_forward = FeedForward(d_model,d_ff,dropout)\n",
        "        decoder_block = DecoderBlock(decoder_self_attention,decoder_cross_attention , feed_forward,dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "    #  obv we project into the target vocab size\n",
        "    projection_layer = ProjectionLayer(d_model,tgt_vocab_size)\n",
        "\n",
        "\n",
        "    transformer = Transformer(encoder,decoder,src_embedding,tgt_embedding,src_pos,tgt_pos,projection_layer)\n",
        "\n",
        "    #  TO MKE THE TRAINING FASTER WE INIT USING XAVIER\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim()>1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eezChYpkFbQc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The tokenizer is what comes before embedding , the goal is to split sentence into single words.\n",
        "# and map each token to a number\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset , DataLoader , random_split\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer # to be researched\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def get_or_build_tokenizer(config,ds,lang):\n",
        "    # config['tokenizer_file'] = '../tokenizer_{0}.e' we use the lang as a param\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    # if not Path.exists(tokenizer_path):\n",
        "    #     tokenizer = Tokenizer(WordLevel(unk_token='[UNK]')) # token for unkown tokens\n",
        "    #     tokenizer.pre_tokenizer = Whitespace()\n",
        "    #     # trainer = WordLevelTrainer(specil_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"],min_frequency=2)\n",
        "    #     trainer = WordLevelTrainer(\n",
        "    #                     special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
        "    #                     min_frequency=2\n",
        "    #                 )\n",
        "\n",
        "    #     tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer=trainer)\n",
        "    #     tokenizer.save(str(tokenizer_path))\n",
        "    # else :\n",
        "    #     tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "\n",
        "\n",
        "\n",
        "    tokenizer = Tokenizer(WordLevel(unk_token='[UNK]')) # token for unkown tokens\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    # trainer = WordLevelTrainer(specil_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"],min_frequency=2)\n",
        "    trainer = WordLevelTrainer(\n",
        "                    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
        "                    min_frequency=2\n",
        "                )\n",
        "\n",
        "    tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer=trainer)\n",
        "    tokenizer.save(str(tokenizer_path))\n",
        "\n",
        "\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def get_all_sentences(ds,lang):\n",
        "    # print(ds)\n",
        "    for item in ds['train'] :\n",
        "        # print(item)\n",
        "        yield item['translation'][lang]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Smnzo4YVMDiZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BiLingualDataset(Dataset):\n",
        "    def __init__(self,ds,tokenizer_src,tokenizer_tgt,src_lang,tgt_lang,seq_len):\n",
        "        super().__init__()\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_src.token_to_id('[SOS]')],dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_src.token_to_id('[EOS]')],dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_src.token_to_id('[PAD]')],dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        src_target_pair = self.ds[index]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        enc_num_pad_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "        dec_num_pad_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "        if(enc_num_pad_tokens < 0 or dec_num_pad_tokens < 0):\n",
        "            raise ValueError('Sentence is too long')\n",
        "\n",
        "        #  add special tokens to src txt\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens,dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token]*enc_num_pad_tokens,dtype=torch.int64)\n",
        "            ]\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "        #\n",
        "        decoder_input = torch.cat([\n",
        "            self.sos_token,\n",
        "            torch.tensor(dec_input_tokens,dtype=torch.int64),\n",
        "            torch.tensor([self.pad_token]*dec_num_pad_tokens,dtype=torch.int64)\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                # sos? we learn to remove the sos ?\n",
        "                torch.tensor(dec_input_tokens,dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token]*dec_num_pad_tokens,dtype=torch.int64)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\":encoder_input,\n",
        "            \"decoder_input\":decoder_input,\n",
        "            # we add the seq dimension and pad dimension (1,1,seq_len)\n",
        "            \"encoder_mask\":(encoder_input!=self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "            \"decoder_mask\":(decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() \\\n",
        "            & self.causal_mask(decoder_input.size(0)),\n",
        "            # (1,seq_len) & (1,seq_len)\n",
        "            \"label\":label,\n",
        "            \"src_text\":src_text,\n",
        "            \"tgt_text\":tgt_text\n",
        "            }\n",
        "\n",
        "\n",
        "    def causal_mask(self,seq_len):\n",
        "        # mask future tokens using 0\n",
        "        mask = torch.triu(torch.ones(1,seq_len,seq_len),diagonal=1).type(torch.int)\n",
        "        return mask == 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vM9za0-N_cil"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_ds(config):\n",
        "    ds_raw = load_dataset('Helsinki-NLP/opus_books',f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}'\n",
        "                        # ,split='train'\n",
        "                          )\n",
        "\n",
        "    tokenizer_src = get_or_build_tokenizer(config,ds_raw,config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config,ds_raw,config['lang_tgt'])\n",
        "\n",
        "    # data splitting\n",
        "    train_ds_size = int(0.9*len(ds_raw['train']))\n",
        "    # val_ds_size = int(0.1*len(ds_raw))\n",
        "    val_ds_size = len(ds_raw['train']) - train_ds_size  # ensures total adds up\n",
        "\n",
        "    train_ds_raw , val_ds_raw = random_split(ds_raw['train'],[train_ds_size,val_ds_size])\n",
        "\n",
        "    # dataset in tensors\n",
        "    train_ds = BiLingualDataset(train_ds_raw,tokenizer_src,tokenizer_tgt,config['lang_src'],config['lang_tgt'],config['seq_len'])\n",
        "    val_ds = BiLingualDataset(val_ds_raw,tokenizer_src,tokenizer_tgt,config['lang_src'],config['lang_tgt'],config['seq_len'])\n",
        "\n",
        "\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    print(f\"Training dataset size: {len(train_ds)}\")\n",
        "    print(f\"Validation dataset size: {len(val_ds)}\")\n",
        "\n",
        "    for item in ds_raw['train']:\n",
        "\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src,len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt,len(tgt_ids))\n",
        "\n",
        "\n",
        "    print(f'max_len_src : {max_len_src}')\n",
        "    print(f'max_len_tgt : {max_len_tgt}')\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds,batch_size=config['batch_size'],shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1,shuffle=True)\n",
        "\n",
        "\n",
        "    return train_dataloader , val_dataloader , tokenizer_src, tokenizer_tgt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_model(config,vocab_src_len,vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len,vocab_tgt_len,config['seq_len'],config['seq_len'],config['d_model'])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LlU881QRpbi9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\":8,\n",
        "        \"num_epochs\":23,\n",
        "        \"lr\":1e-4,\n",
        "        \"seq_len\":512,\n",
        "        \"d_model\":512,\n",
        "        \"lang_src\":\"en\",\n",
        "        \"lang_tgt\":\"fr\",\n",
        "        \"model_folder\":\"weights\",\n",
        "        \"model_filenme\":\"smodel\",\n",
        "        \"preload\":None,\n",
        "        \"tokenizer_file\":\"tokenizer_{0}.json\",\n",
        "        \"experiment_name\":\"runs/smodel\",\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def get_weights_file_path(config,epoch):\n",
        "    model_folder = config['model_folder']\n",
        "    model_basename = config['model_basenme']\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
        "    return str(Path('.')/model_folder/model_filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "czlC2sTcrpsf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(config):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Device : {device}')\n",
        "\n",
        "    Path(config['model_folder']).mkdir(parents=True,exist_ok=True)\n",
        "    train_dataloader , val_dataloader , tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "\n",
        "    model = get_model(config,tokenizer_src.get_vocab_size(),tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=config['lr'],eps=1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    if config['preload']:\n",
        "        model_filename = get_weights_file_path(config,config['preload'])\n",
        "        print(f'PreLOADING')\n",
        "        state = torch.load(model_filename)\n",
        "        initial_epoch = state['epoch']+1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'),label_smoothing=0.1).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader,desc=f'Epoch:{epoch}')\n",
        "\n",
        "        for batch in batch_iterator :\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device) # (8,1,1,seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device) # (8,1,1,seq_len,seq_len)\n",
        "\n",
        "            encoder_output = model.encode(encoder_input,encoder_mask)\n",
        "            decoder_output = model.decode(encoder_output , encoder_mask ,decoder_input,decoder_mask)\n",
        "\n",
        "            projection_output = model.project(decoder_output)\n",
        "\n",
        "\n",
        "            label = batch['label'].to(device) # (B,seq_len)\n",
        "            # (B,seq_len,tgt_v_size) --> (B * seq_len , tgt_v_size)\n",
        "            loss = loss_fn(projection_output.view(-1,tokenizer_tgt.get_vocab_size()),label.view(-1))\n",
        "\n",
        "            batch_iterator.set_postfix({f\"loss \":f\"{loss.item():6.3f}\"})\n",
        "            writer.add_scalar('train loss',loss.item(),global_step)\n",
        "            writer.flush()\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_step +=1\n",
        "\n",
        "        model_filename = get_weights_file_path(config,f'{epoch}')\n",
        "\n",
        "        torch.save({\n",
        "            'epoch':epoch,\n",
        "            'model_state_dict':model.state_dict(),\n",
        "            'optimizer_state_dict':optimizer.state_dict(),\n",
        "            'global_step':global_step\n",
        "            },model_filename)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cXaWjIxvugVf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tokenizers transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k2eqav_zVaJ",
        "outputId": "ee324027-eb79-40b6-c061-baa219b1b425"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli login\n"
      ],
      "metadata": {
        "id": "GLZrN8Ai1r8m"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U datasets fsspec huggingface_hub\n"
      ],
      "metadata": {
        "id": "kGa_OC1m4uAi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "config = get_config()\n",
        "train_model(config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pua0VxDpx06x",
        "outputId": "7616d7f7-303a-4754-aaea-cb526b966994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device : cuda\n",
            "Training dataset size: 114376\n",
            "Validation dataset size: 12709\n",
            "max_len_src : 471\n",
            "max_len_tgt : 482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:0:   1%|▏         | 187/14297 [02:02<2:32:52,  1.54it/s, loss =6.997]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "bGdaatUwbCMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# ds = load_dataset(\"Helsinki-NLP/opus_books\", \"ca-de\")"
      ],
      "metadata": {
        "id": "dQtfsALC1gv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import get_dataset_config_names\n",
        "\n",
        "# print(get_dataset_config_names(\"Helsinki-NLP/opus_books\"))"
      ],
      "metadata": {
        "id": "xEzx68JH3fE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import HfApi\n",
        "\n",
        "# api = HfApi()\n",
        "# dataset_info = api.dataset_info(\"Helsinki-NLP/opus_books\")\n",
        "# print(dataset_info)\n",
        "# # configs = [config.id for config in dataset_info.siblings if config.id.endswith(\".json\")]\n",
        "\n",
        "# # print(\"Available configs (language pairs):\")\n",
        "# # for cfg in configs:\n",
        "# #     print(cfg.replace(\".json\", \"\"))"
      ],
      "metadata": {
        "id": "WFvOXmUa3_fT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}